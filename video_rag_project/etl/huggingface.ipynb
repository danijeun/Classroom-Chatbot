{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwebdataset\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwds\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import webdataset as wds\n",
    "from pymongo import MongoClient\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MONGO_URI = \"mongodb://localhost:27017\"  # Change to your MongoDB URI\n",
    "DB_NAME = \"video_lectures\"\n",
    "COLLECTION_NAME = \"frames\"\n",
    "REPO_ID = \"aegean-ai/ai-lectures-spring-24\"\n",
    "TAR_FILENAME = \"youtube_dataset.tar\"\n",
    "DOWNLOAD_DIR = \"./data\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "class WebDatasetToMongoDB:\n",
    "    def __init__(self, mongo_uri, db_name, collection_name):\n",
    "        self.client = MongoClient(mongo_uri)\n",
    "        self.db = self.client[db_name]\n",
    "        self.collection = self.db[collection_name]\n",
    "        self.prev_frame = None\n",
    "        self.current_subtitle = None\n",
    "        self.similarity_threshold = 0.9\n",
    "        self._create_indexes()\n",
    "\n",
    "    def _create_indexes(self):\n",
    "        \"\"\"Create indexes for efficient querying\"\"\"\n",
    "        self.collection.create_index(\"timestamp\")\n",
    "        self.collection.create_index([(\"subtitle\", \"text\")])\n",
    "        self.collection.create_index(\"video_id\")\n",
    "        self.collection.create_index(\"frame_hash\", unique=True)\n",
    "\n",
    "    def _frame_to_hash(self, frame_array):\n",
    "        \"\"\"Generate hash for frame content\"\"\"\n",
    "        gray = cv2.cvtColor(frame_array, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, (16, 16))\n",
    "        return str(resized.tobytes())\n",
    "\n",
    "    def _is_similar_frame(self, frame1, frame2):\n",
    "        \"\"\"Check frame similarity using SSIM\"\"\"\n",
    "        if frame1 is None or frame2 is None:\n",
    "            return False\n",
    "            \n",
    "        gray1 = cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY)\n",
    "        gray2 = cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY)\n",
    "        gray1 = cv2.resize(gray1, (64, 64))\n",
    "        gray2 = cv2.resize(gray2, (64, 64))\n",
    "        score, _ = structural_similarity(gray1, gray2, full=True)\n",
    "        return score > self.similarity_threshold\n",
    "\n",
    "    def _process_sample(self, sample):\n",
    "        \"\"\"Process a WebDataset sample\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = json.loads(sample['metadata.json'].decode('utf-8')) if 'metadata.json' in sample else {}\n",
    "        video_id = metadata.get('video_id', 'unknown')\n",
    "        \n",
    "        # Get current subtitle\n",
    "        current_subtitle = sample.get('subtitle.txt', b'').decode('utf-8').strip()\n",
    "        \n",
    "        # Process all frames\n",
    "        for key in [k for k in sample.keys() if k.startswith('frame_')]:\n",
    "            try:\n",
    "                frame_data = sample[key]\n",
    "                frame = Image.open(io.BytesIO(frame_data))\n",
    "                frame_array = np.array(frame)\n",
    "                timestamp = float(key.split('_')[1].split('.')[0])\n",
    "                frame_hash = self._frame_to_hash(frame_array)\n",
    "                \n",
    "                if not self._is_similar_frame(frame_array, self.prev_frame):\n",
    "                    document = {\n",
    "                        'video_id': video_id,\n",
    "                        'timestamp': timestamp,\n",
    "                        'frame_data': frame_data,\n",
    "                        'frame_format': 'png',\n",
    "                        'frame_width': frame.width,\n",
    "                        'frame_height': frame.height,\n",
    "                        'frame_hash': frame_hash,\n",
    "                        'subtitle': current_subtitle,\n",
    "                        'metadata': metadata,\n",
    "                        'processing_date': datetime.utcnow()\n",
    "                    }\n",
    "                    documents.append(document)\n",
    "                    self.prev_frame = frame_array\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {key}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return documents\n",
    "\n",
    "    def process_webdataset(self, tar_path, batch_size=100):\n",
    "        \"\"\"Process downloaded WebDataset\"\"\"\n",
    "        dataset = wds.WebDataset(tar_path).decode(wds.autodecode.ImageHandler(\"rgb\"))\n",
    "        \n",
    "        total_processed = 0\n",
    "        batch = []\n",
    "        \n",
    "        for sample in dataset:\n",
    "            documents = self._process_sample(sample)\n",
    "            batch.extend(documents)\n",
    "            \n",
    "            if len(batch) >= batch_size:\n",
    "                try:\n",
    "                    result = self.collection.insert_many(batch, ordered=False)\n",
    "                    total_processed += len(result.inserted_ids)\n",
    "                    batch = []\n",
    "                    print(f\"Processed {total_processed} documents\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Batch insert error: {str(e)}\")\n",
    "                    # Fallback to single inserts\n",
    "                    for doc in batch:\n",
    "                        try:\n",
    "                            self.collection.insert_one(doc)\n",
    "                            total_processed += 1\n",
    "                        except:\n",
    "                            continue\n",
    "                    batch = []\n",
    "        \n",
    "        if batch:\n",
    "            try:\n",
    "                result = self.collection.insert_many(batch, ordered=False)\n",
    "                total_processed += len(result.inserted_ids)\n",
    "            except Exception as e:\n",
    "                print(f\"Final batch insert error: {str(e)}\")\n",
    "        \n",
    "        print(f\"Processing complete. Total documents inserted: {total_processed}\")\n",
    "        return total_processed\n",
    "\n",
    "# Download the dataset\n",
    "print(\"Downloading dataset...\")\n",
    "tar_path = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=TAR_FILENAME,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=DOWNLOAD_DIR,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "print(f\"Dataset downloaded to: {tar_path}\")\n",
    "\n",
    "# Process and store in MongoDB\n",
    "print(\"Processing dataset and storing in MongoDB...\")\n",
    "processor = WebDatasetToMongoDB(\n",
    "    mongo_uri=MONGO_URI,\n",
    "    db_name=DB_NAME,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "processor.process_webdataset(\n",
    "    tar_path=tar_path,\n",
    "    batch_size=200\n",
    ")\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
